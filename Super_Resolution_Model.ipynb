{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJtAGjxQDiku",
        "outputId": "9e240e20-bfcf-43e4-b10d-2143bee10c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting super_image\n",
            "  Downloading super_image-0.1.7-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: h5py>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from super_image) (3.11.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from super_image) (0.24.6)\n",
            "Requirement already satisfied: opencv-python>=4.5.2.54 in /usr/local/lib/python3.10/dist-packages (from super_image) (4.10.0.84)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from super_image) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from super_image) (0.19.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.61.2 in /usr/local/lib/python3.10/dist-packages (from super_image) (4.66.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py>=3.1.0->super_image) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.13->super_image) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.13->super_image) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.13->super_image) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.13->super_image) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.13->super_image) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.13->super_image) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->super_image) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->super_image) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->super_image) (3.1.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.0->super_image) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->super_image) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.0.13->super_image) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.0.13->super_image) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.0.13->super_image) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.0.13->super_image) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->super_image) (1.3.0)\n",
            "Downloading super_image-0.1.7-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: super_image\n",
            "Successfully installed super_image-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install super_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNks4ni-26-L"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "from super_image import CarnModel, ImageLoader\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHcYHqk023Q4",
        "outputId": "ee6acfb0-e91c-4f18-db0a-3a758a26c0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://huggingface.co/eugenesiow/carn-bam/resolve/main/pytorch_model_4x.pt\n",
            "Video processing complete with CarnModel. The enhanced video is saved as: D02_20240705162958 (online-video-cutter.com) (8) output.mp4\n"
          ]
        }
      ],
      "source": [
        "# Load the CarnModel for super-resolution with scale 4\n",
        "model = CarnModel.from_pretrained('eugenesiow/carn-bam', scale=4)  # You can adjust the scale as needed\n",
        "\n",
        "# Load the video\n",
        "input_video_path = \"D02_20240705162958 (online-video-cutter.com) (8).mp4\"\n",
        "output_video_path = \"D02_20240705162958 (online-video-cutter.com) (8) output.mp4\"\n",
        "\n",
        "# Load the pre-trained face detector (Haar Cascade)\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Open the input video\n",
        "video = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "# Get the frame width, height, and FPS\n",
        "frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Prepare the output video writer\n",
        "output_video = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
        "\n",
        "# Define a tensor-to-PIL conversion utility\n",
        "to_pil_image = transforms.ToPILImage()\n",
        "\n",
        "# Loop through each frame in the video\n",
        "while True:\n",
        "    ret, frame = video.read()\n",
        "    if not ret:\n",
        "        break  # Exit loop if there are no more frames\n",
        "\n",
        "    # Convert the frame to grayscale for face detection\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    if len(faces) > 0:\n",
        "        # Crop the first detected face\n",
        "        x, y, w, h = faces[0]  # Assuming the first detected face is the one we want to crop\n",
        "        face_frame = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Convert the cropped face to a PIL Image for processing\n",
        "        pil_image = Image.fromarray(cv2.cvtColor(face_frame, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for PIL\n",
        "\n",
        "        # Load the image as input for the model\n",
        "        inputs = ImageLoader.load_image(pil_image)\n",
        "\n",
        "        # Apply super resolution using CarnModel\n",
        "        with torch.no_grad():\n",
        "            preds = model(inputs)\n",
        "\n",
        "        # Convert the tensor output back to a PIL image using torchvision\n",
        "        sr_image = to_pil_image(preds.squeeze(0))\n",
        "\n",
        "        # Convert the PIL image back to OpenCV format (BGR)\n",
        "        sr_image = np.array(sr_image)\n",
        "        sr_image = cv2.cvtColor(sr_image, cv2.COLOR_RGB2BGR)  # Convert RGB back to BGR\n",
        "\n",
        "        # Resize the upscaled face frame to fit the original frame size (optional)\n",
        "        resized_face_frame = cv2.resize(sr_image, (frame_width, frame_height))\n",
        "\n",
        "        # Write the resized face frame to the output video\n",
        "        output_video.write(resized_face_frame)\n",
        "    else:\n",
        "        # Write the original frame if no face is detected\n",
        "        output_video.write(frame)\n",
        "\n",
        "# Release the video objects\n",
        "video.release()\n",
        "output_video.release()\n",
        "\n",
        "print(\"Video processing complete with CarnModel. The enhanced video is saved as:\", output_video_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zFyuupcP3d5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}